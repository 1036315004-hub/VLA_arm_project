# Automated Data Collection for VLA Arm Project

This directory contains the automated data collection system for training VLA (Vision-Language-Action) models.

## Overview

The system collects RGB-D sequences with supervision labels generated by an Oracle (ground truth driver). Each episode includes:
- **Input**: Camera1 RGB-D sequences (256×256) + templated instruction
- **Output**: 4 keyframe end-effector poses in robot base coordinates

## Keyframes

The fixed keyframe sequence (in order):
1. **hover**: Initial approach position at fixed height (Z=0.65m)
2. **pre_contact**: Positioned 4cm above contact point
3. **contact**: At the AABB top center (gras_pos from Oracle)
4. **lift**: Contact point + 10cm lift

## Usage

### Basic Collection
```bash
cd VLA_arm_project/scripts
python auto_collect.py --num_episodes 100 --save_dir data/keyframe_demos
```

### With GUI Visualization
```bash
python auto_collect.py --num_episodes 10 --gui
```

### Full Options
```bash
python auto_collect.py \
    --num_episodes 100 \
    --save_dir data/keyframe_demos \
    --gui \
    --seed 42 \
    --record_stride 5 \
    --max_steps_per_phase 150
```

### Inspect Episode
```bash
python auto_collect.py --save_dir data/keyframe_demos --inspect 0
```

## CLI Arguments

| Argument | Type | Default | Description |
|----------|------|---------|-------------|
| `--num_episodes` | int | 10 | Number of episodes to collect |
| `--save_dir` | str | `data/keyframe_demos` | Save directory (relative to project root) |
| `--gui` | flag | False | Enable PyBullet GUI visualization |
| `--seed` | int | None | Random seed for reproducibility |
| `--record_stride` | int | 5 | Record frame every N simulation steps |
| `--max_steps_per_phase` | int | 150 | Max steps per motion phase |
| `--inspect` | int | None | Inspect specific episode instead of collecting |

## Output Structure

```
data/keyframe_demos/
├── images/
│   ├── ep_0_step_0.png
│   ├── ep_0_step_1.png
│   └── ...
├── depth/
│   ├── ep_0_step_0_depth.npy
│   ├── ep_0_step_1_depth.npy
│   └── ...
├── metadata/
│   ├── episode_0.json
│   ├── episode_1.json
│   └── ...
└── collection_stats.json
```

## Episode Metadata Format

Each `episode_N.json` contains:

```json
{
    "episode_id": 0,
    "instruction": "touch the blue cup",
    "timestamp": 1234567890.123,
    "success": true,
    "accepted": true,
    "num_steps": 45,
    
    "camera": {
        "eye": [0.60, 0.60, 0.90],
        "target": [0.80, 0.00, 0.40],
        "up": [0, 0, 1],
        "fov": 60,
        "near": 0.1,
        "far": 2.0,
        "width": 256,
        "height": 256,
        "view_matrix": [...],
        "proj_matrix": [...]
    },
    
    "robot": {
        "base_world_pose": {
            "position": [0.40, 0.00, 0.40],
            "orientation": [0, 0, 0.707, 0.707]
        },
        "ee_pose_definition": "ee_pose_base = T_base_world^(-1) * ee_pose_world"
    },
    
    "target": {
        "object_id": 5,
        "name": "blue_cup_1",
        "gras_pos_world": [0.75, 0.10, 0.45]
    },
    
    "keyframes": [
        {
            "name": "hover",
            "step_id": 10,
            "ee_position_base": [...],
            "ee_orientation_base": [...],
            "ee_position_world": [...],
            "ee_orientation_world": [...],
            "target_position_world": [...],
            "convergence_distance": 0.008
        },
        // ... pre_contact, contact, lift
    ],
    
    "steps": [
        {
            "step_id": 0,
            "image_path": "images/ep_0_step_0.png",
            "depth_path": "depth/ep_0_step_0_depth.npy",
            "joint_positions": [...],
            "joint_velocities": [...],
            "ee_position_world": [...],
            "ee_orientation_world": [...],
            "ee_position_base": [...],
            "ee_orientation_base": [...],
            "phase": "hover"
        },
        // ...
    ],
    
    "quality": {
        "accepted": true,
        "reasons": [],
        "metrics": {
            "keyframes_present": ["hover", "pre_contact", "contact", "lift"],
            "keyframes_missing": [],
            "convergence_distances": {"hover": 0.008, ...},
            "contact_detected": true,
            "depth_valid": {"hover": true, ...},
            "num_steps": 45
        }
    }
}
```

## Quality Gates

Each episode is validated against multiple quality gates. Only episodes passing ALL gates are marked as `accepted=true`.

### Mandatory Gates (Hard Rejection)

| Gate | Description | Threshold |
|------|-------------|-----------|
| **A** | All 4 keyframes exist | hover, pre_contact, contact, lift |
| **B** | Keyframe convergence | hover < 0.015m, pre_contact < 0.010m, contact < 0.008m, lift < 0.015m |
| **C** | Contact detection | Must have contact at contact phase |
| **D** | Valid depth | No NaN/Inf, center region has valid values |
| **E** | Minimum sequence length | ≥ 20 steps |

### Optional Gate

| Gate | Description | Threshold |
|------|-------------|-----------|
| **F** | Target mask area | ≥ 150 pixels (configurable) |

## Training Data Construction

For training, sample from **accepted** episodes only:

### Input
- Extract L frames (e.g., L=8 or L=12) uniformly sampled from hover→contact
- Include keyframe-adjacent frames
- Each frame: RGB (256×256) + Depth (256×256)
- Instruction text

### Output (Supervision Labels)
- 4 keyframe poses in base coordinates (position + quaternion)
- Fixed order: [hover, pre_contact, contact, lift]

### Example PyTorch DataLoader Pattern

```python
import json
import numpy as np
from PIL import Image

def load_episode(meta_path):
    with open(meta_path) as f:
        data = json.load(f)
    
    # Skip rejected episodes
    if not data.get('accepted', False):
        return None
    
    # Extract keyframe poses (base frame)
    keyframes = {}
    for kf in data['keyframes']:
        keyframes[kf['name']] = {
            'pos': np.array(kf['ee_position_base']),
            'orn': np.array(kf['ee_orientation_base'])
        }
    
    # Sample frames
    steps = data['steps']
    # ... implement uniform sampling
    
    return {
        'instruction': data['instruction'],
        'keyframes': keyframes,
        'frames': frames  # List of (rgb, depth) tuples
    }
```

## Fixed Parameters

| Parameter | Value | Description |
|-----------|-------|-------------|
| Camera resolution | 256×256 | RGB and Depth |
| Camera eye | [0.60, 0.60, 0.90] | Fixed position |
| Camera target | [0.80, 0.00, 0.40] | Look-at point |
| Camera FOV | 60° | Field of view |
| Hover height | 0.65m | Z-coordinate |
| Pre-contact offset | 0.04m | Above contact |
| Lift delta | 0.10m | Above contact |
| Robot base | [0.40, 0.00, 0.40] | Fixed position |

## Coordinate Systems

- **World frame**: PyBullet world coordinates
- **Base frame**: Robot base coordinates (used for keyframe labels)
- **Transform**: `T_base_world = (base_position, base_orientation)`
- **Conversion**: `pose_base = T_base_world^(-1) * pose_world`

## Statistics

After collection, `collection_stats.json` contains:
```json
{
    "total": 100,
    "accepted": 85,
    "rejected": 15,
    "success": 90,
    "rejection_reasons": {
        "Gate B": 5,
        "Gate C": 8,
        "Gate D": 2
    }
}
```
